{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_O5AFSI--j_V"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ajpar94/embeddings-comparison/blob/master/notebooks/train_flair_embedding.ipynb)\n",
    "# Training a Flair Language Model\n",
    "\n",
    "Example code for training your own [flair](https://github.com/zalandoresearch/flair) language models (*Flair Embeddings*). The majority of this content is similar/equal to [Tutorial: Training your own Flair Embeddings](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md), so make sure to check it out for greater detail.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjRfOBUpDGzI"
   },
   "source": [
    "### Google Colab setup\n",
    "First, make sure to turn on 'Hardware accelerator: GPU' in *Edit > Notebook Settings*. Next, we will we mount our google drive to easily access corpora, datasets, embeddings and store models. Finally, install flair and configure your paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hPJQQIpoG6K8"
   },
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "N0GbVu-5Cx1M"
   },
   "outputs": [],
   "source": [
    "!pip install flair --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QHS2J4LLCReP"
   },
   "outputs": [],
   "source": [
    "# PATHS\n",
    "base_path = \"/gdrive/My Drive/WordEmbeddings-Comparison/\"\n",
    "lm_path = f\"{base_path}Language-Modeling/resources/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5hJHvAwy2Es"
   },
   "source": [
    "## Preparing the Corpus\n",
    "To train your own embeddings you need a suitably large plain text file. The Corpus must consist of following parts:\n",
    "\n",
    "*   Test data -> *'test.txt'*\n",
    "*   Validation data -> *'valid.txt'*\n",
    "*   Training data splitted in to many smaller parts contained in the folder *'train'*\n",
    "\n",
    "This means the corpus folder structure has to look like this:\n",
    "\n",
    "```console\n",
    "corpus\n",
    " |- test.txt\n",
    " |- valid.txt\n",
    " |- train\n",
    "     |- train_split_1\n",
    "     |- train_split_2\n",
    "     |  ...\n",
    "     |- train_split_x\n",
    "```\n",
    " \n",
    "To create a corpus folder from one plain text file, you can use this script: [*make_corpus_folder.py*](https://github.com/ajpar94/embeddings-comparison/blob/master/Language-Modeling/preprocessing/make_corpus_folder.py). For example, if you want use 1% of the data for validation, 2% for testing, 97% for training and want the training data to be splitted in 20 parts, you can do\n",
    "\n",
    "```console\n",
    "$ python make_corpus_folder.py corpus.txt /path/to/corpus_folder -p 97-1-2 -s 20\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4xU2UMJ6BPR"
   },
   "source": [
    "## Training the Language Model\n",
    "You need to specify whether you want to train a forward or a backward model (I would recommend doing both). If you will use a Latin alphabet, load the default character dictionary. For Non-Latin alphabets check the [tutorial](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md#non-latin-alphabets) on how create your own character dictionary. Intialize your *TextCorpus*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hgDPjZ0GN6XS"
   },
   "outputs": [],
   "source": [
    "from flair.data import Dictionary\n",
    "from flair.models import LanguageModel\n",
    "from flair.trainers.language_model_trainer import TextCorpus\n",
    "\n",
    "# are you training a forward or backward LM?\n",
    "is_forward_lm = True\n",
    "\n",
    "# load the default character dictionary\n",
    "dictionary = Dictionary.load('chars')\n",
    "\n",
    "# corpus folder with train splits, test and valid\n",
    "corpus_folder \"example-corpus\"\n",
    "corpus_path = f\"{lm_path}corpus/{corpus_folder}/\n",
    "\n",
    "# initialize corpus\n",
    "corpus = TextCorpus(corpus_path,\n",
    "                    dictionary,\n",
    "                    is_forward_lm,\n",
    "                    character_level=True,\n",
    "                    random_case_flip=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_C1IBTOPTNUU"
   },
   "source": [
    "Next, specify the model folder. After training, this folder will contain the best model, the final model checkpoint, checkpoints for every epoch, training.log and a loss.txt (details about the training process).\n",
    "\n",
    "Training a decent language model will require a powerful decent GPU and probably a lot of time. However, you can quit training whenever you want a continue training from a checkpoint (either `checkpoint.pt` or `epoch_X.pt`). The training log will show you the progress of the training process, incl. current loss, perplexity and learning rate. After each epoch the log will display a sequence of text generated by the current model. The similarity of this text to real language can be seen as an indication for how well the model has learned. The model is trained with an annealed learning rate. Setting `patience=10` means the scheduler will decrease the learning rate after 10 splits without improvement. For a full list of training parameters check [here](https://github.com/zalandoresearch/flair/blob/master/flair/trainers/language_model_trainer.py#L244)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hMNoerT-qFba"
   },
   "outputs": [],
   "source": [
    "from flair.trainers.language_model_trainer import LanguageModelTrainer\n",
    "\n",
    "# model folder\n",
    "model_folder = \"example-fwd\"\n",
    "model_path = f\"{lm_path}models/FLAIR/{model_folder}/\"\n",
    "\n",
    "# option to continue training from checkpoint\n",
    "continue_training = False\n",
    "\n",
    "if not continue_training:\n",
    "    # instantiate your language model, set hidden size and number of layers\n",
    "    language_model = LanguageModel(dictionary,\n",
    "                                   is_forward_lm,\n",
    "                                   hidden_size=1024,\n",
    "                                   nlayers=1)\n",
    "  \n",
    "    trainer = LanguageModelTrainer(language_model, corpus)\n",
    "  \n",
    "else:\n",
    "    checkpoint = f\"{model_path}checkpoint.pt\"\n",
    "    trainer = LanguageModelTrainer.load_from_checkpoint(checkpoint, corpus)\n",
    "\n",
    "\n",
    "trainer.log_interval = 500\n",
    "trainer.train(model_path,\n",
    "              sequence_length=250,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=10,\n",
    "              learning_rate=20.0,\n",
    "              patience=10,\n",
    "              checkpoint=True,\n",
    "              num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oepXdj6oi5Bu"
   },
   "source": [
    "## Fine-Tuning an Existing Language Model\n",
    "\n",
    "Fine-tuning an existing model can be easier than training from scratch. For example, if you have a general LM for German and you would like to fine-tune for a specific domain. You can access the language model like this: `FlairEmbeddings('de-forward').lm`\n",
    "\n",
    "Keep in mind that you need to match the direction of the model and dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VS-JKXB8kujU"
   },
   "outputs": [],
   "source": [
    "from flair.data import Dictionary\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus\n",
    "\n",
    "\n",
    "# instantiate an existing LM, such as one from the FlairEmbeddings\n",
    "language_model = FlairEmbeddings('de-forward').lm\n",
    "\n",
    "# are you fine-tuning a forward or backward LM?\n",
    "is_forward_lm = language_model.is_forward_lm\n",
    "\n",
    "# get the dictionary from the existing language model\n",
    "dictionary: Dictionary = language_model.dictionary\n",
    "\n",
    "# corpus folder with train splits, test and valid\n",
    "corpus_folder \"example-corpus\"\n",
    "corpus_path = f\"{lm_path}corpus/{corpus_folder}/\n",
    "\n",
    "# initialize Corpus\n",
    "corpus = TextCorpus(corpus_path,\n",
    "                    dictionary,\n",
    "                    is_forward_lm,\n",
    "                    character_level=True)\n",
    "\n",
    "# use the model trainer to fine-tune this model on your corpus\n",
    "trainer = LanguageModelTrainer(language_model, corpus)\n",
    "\n",
    "# model folder\n",
    "model_folder = \"de-forward-finetuned\"\n",
    "model_path = f\"{lm_path}models/FLAIR/{model_folder}/\"\n",
    "\n",
    "trainer.train(model_path,\n",
    "              sequence_length=100,\n",
    "              mini_batch_size=100,\n",
    "              learning_rate=20,\n",
    "              patience=10,\n",
    "              checkpoint=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train-flair-embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
