{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_O5AFSI--j_V"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ajpar94/embeddings-comparison/blob/master/notebooks/named_entity_recognition.ipynb)\n",
    "# Training a Sequence Labeling Model (Named-Entity-Recognition)\n",
    "\n",
    "Example code for training a NER model with [flair](https://github.com/zalandoresearch/flair). See also [Tutorial: Training a Model](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md).\n",
    "\n",
    "This notebook will show how to train a NER model with a **Train/Validation/Test split** and with **K-Fold Cross-Validation** .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjRfOBUpDGzI"
   },
   "source": [
    "### Google Colab setup\n",
    "First, make sure to turn on 'Hardware accelerator: GPU' in *Edit > Notebook Settings*. Next, we will we mount our google drive to easily access corpora, datasets, embeddings and store models. Finally, install flair and configure your paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hPJQQIpoG6K8"
   },
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "N0GbVu-5Cx1M"
   },
   "outputs": [],
   "source": [
    "!pip install flair --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QHS2J4LLCReP"
   },
   "outputs": [],
   "source": [
    "# PATHS\n",
    "base_path = \"/gdrive/My Drive/WordEmbeddings-Comparison/\"\n",
    "lm_path = f\"{base_path}Language-Modeling/resources/\"\n",
    "ner_path = f\"{base_path}Named-Entity-Recognition/resources/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5hJHvAwy2Es"
   },
   "source": [
    "---\n",
    "## Variant 1 - Training/Validation/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mkf7BFpvF-We"
   },
   "source": [
    "### Sequence Labeling Dataset (Corpus)\n",
    "A *ColumnCorpus* consists out of tagged sentences and is constructed by a file in column format where each line has one word together with its linguistic annotation. Sentences are seperated by blank line. Example:\n",
    "\n",
    "```text\n",
    "James B-person\n",
    "Watson I-person\n",
    "visited O\n",
    "Germany B-country\n",
    "in O\n",
    "2019 B-year\n",
    ". O\n",
    "\n",
    "Sam B-person\n",
    "was O\n",
    "not B-negation\n",
    "there O\n",
    ". O\n",
    "\n",
    "```\n",
    "\n",
    "In our example the second column represents a ner tag in BIO format. You need three of those files: train, dev, test, which correspond to the training, validation and testing split during model training. You can also split one file by percentages using this [*build_dataset_ner.py*](https://github.com/ajpar94/embeddings-comparison/blob/master/Named-Entity-Recognition/preprocessing/build_dataset_NER.py). Alternatively, use one of flair's prepared datasets. Define the *ColumnCorpus*, define what *tag* to predict and create a *tag_dictionary*. See also [Tutorial: Loading Training Data](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Kk9pwnBDF_YN"
   },
   "outputs": [],
   "source": [
    "# PREPARE CORPUS\n",
    "# alternative: from flair.datasets import WIKINER_ENGLISH\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns (multiple possible: ... 2: 'pos')\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "corpus_folder = f\"{ner_path}datasets/tagged/\"\n",
    "\n",
    "# init a corpus using column format, data folder \n",
    "# alternative: corpus = WIKINER_ENGLISH()\n",
    "corpus = ColumnCorpus(corpus_folder, columns,\n",
    "                      train_file='train.txt',\n",
    "                      test_file='test.txt',\n",
    "                      dev_file='dev.txt')\n",
    "print(corpus)\n",
    "\n",
    "# what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary.idx2item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KFk1vkUGaVz"
   },
   "source": [
    "### Embeddings\n",
    "flair comes with many embeddings out of the box (see: [Tutorial: List of All Word Embeddings](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md)). Or point to your own custom embeddings. If you want to know how to train your own embeddings, check [Notebook: Training a Flair Language Model](#) and [Tutorial: Training your own Flair Embeddings](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md).\n",
    "\n",
    "*StackedEmbeddings* can be used combine multiple embeddings, which makes sense when you have a forward and a backward language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zfixbaVMGa0P"
   },
   "outputs": [],
   "source": [
    "# INITIALIZE EMBEDDINGS\n",
    "from flair.embeddings import FlairEmbeddings, StackedEmbeddings\n",
    "\n",
    "# path to embeddings\n",
    "lm_fwd = f\"{lm_path}models/FLAIR/clean_fwd/best-lm.pt\"\n",
    "lm_bwd = f\"{lm_path}models/FLAIR/clean_bwd/best-lm.pt\"\n",
    "\n",
    "embeddings = StackedEmbeddings([FlairEmbeddings(lm_fwd), FlairEmbeddings(lm_bwd)])\n",
    "# alternative: embeddings = StackedEmbeddings([FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bvEphymdGoax"
   },
   "source": [
    "### Sequence Tagger (NER Model)\n",
    "The *SequenceTagger* can take a lot more parameter (e.g. dropout). For a full list, check [here](https://github.com/zalandoresearch/flair/blob/master/flair/models/sequence_tagger_model.py#L68)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cx8OYHSdGoo7"
   },
   "outputs": [],
   "source": [
    "# INITIALIZE SEQUENCE TAGGER\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger = SequenceTagger(hidden_size=512,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=tag_dictionary,\n",
    "                        tag_type=tag_type,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbxfIPtuGxuT"
   },
   "source": [
    "### Model Trainer\n",
    "Define the path to the output/model folder. After training, this folder will usually contain:\n",
    "\n",
    "\n",
    "*   final-model.pt\n",
    "*   checkpoint.pt\n",
    "*   weights.txt\n",
    "*   loss.tsv\n",
    "*   test.tsv\n",
    "*   training.log\n",
    "\n",
    "Depending on whether or not you *train_with_dev* there will a **best-model.pt** aswell. *ModelTrainer.train()* can take a lot of optional parameters. For a full list of parameters, check [here](https://github.com/zalandoresearch/flair/blob/master/flair/trainers/trainer.py#L61).\n",
    "\n",
    "At the end of the *training.log* you will see the relevant metrics including the final F1 score a classification report. For further details on how to perform an evaluation for such a model, check [Notebook: Evaluating a Sequence Labeling Model](#).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F6JWBQEGGyEG"
   },
   "outputs": [],
   "source": [
    "# INITIALIZE TRAINER\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# define output path\n",
    "model_folder = \"ner-model-test\"\n",
    "model_path = f\"{ner_path}models/{model_folder}/\"\n",
    "\n",
    "# option to continue from checkpoint\n",
    "continue_training = False\n",
    "\n",
    "if continue_training:\n",
    "    checkpoint = tagger.load_checkpoint(model_path + 'checkpoint.pt')\n",
    "    trainer = ModelTrainer.load_from_checkpoint(checkpoint, corpus)\n",
    "else:\n",
    "    trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# Training\n",
    "trainer.train(model_path,\n",
    "              learning_rate=0.5,\n",
    "              anneal_factor=0.5,\n",
    "              mini_batch_size=8,\n",
    "              patience=5,\n",
    "              max_epochs=2,\n",
    "              train_with_dev=True,\n",
    "              monitor_test=True,\n",
    "              shuffle=True,\n",
    "              checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZpSaZp7vkcJ"
   },
   "source": [
    "---\n",
    "## Variant 2 - K-Fold Cross-Validation\n",
    "This section explains how to simulate 10-Fold Cross-Validation (CV) while training the ner model. CV is useful, when you want to reliably evaluate how a specific model configuration performs, when you do not have a specific test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oFeRjppzXbR"
   },
   "source": [
    "### Sequence Labeling Dataset (Corpus)\n",
    "Instead of using train/dev/test files, we will initialize our *ColumnCorpus*\n",
    "with the complete file as train, and empty files for dev and test. Lastly, we will save *corpus.train* as numpy array called *TRAIN*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GEhlAh871HZX"
   },
   "outputs": [],
   "source": [
    "# PREPARE CORPUS\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns (multiple possible: ... 2: 'pos')\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "corpus_folder = f\"{ner_path}datasets/tagged/\"\n",
    "\n",
    "# init a corpus using column format, data folder \n",
    "corpus = ColumnCorpus(corpus_folder, columns,\n",
    "                      train_file='tagged.txt',\n",
    "                      test_file='empty.txt',\n",
    "                      dev_file='empty.txt')\n",
    "print(corpus)\n",
    "\n",
    "# what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "#print(tag_dictionary.idx2item)\n",
    "\n",
    "import numpy as np\n",
    "TRAIN = np.array(corpus.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVqseAYF1p27"
   },
   "source": [
    "### Embeddings\n",
    "Same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "A-yBG6yf14Ew"
   },
   "outputs": [],
   "source": [
    "# INITIALIZE EMBEDDINGS\n",
    "from flair.embeddings import FlairEmbeddings, StackedEmbeddings\n",
    "\n",
    "# path to embeddings\n",
    "lm_fwd = f\"{lm_path}models/FLAIR/clean_fwd/best-lm.pt\"\n",
    "lm_bwd = f\"{lm_path}models/FLAIR/clean_bwd/best-lm.pt\"\n",
    "\n",
    "embeddings = StackedEmbeddings([FlairEmbeddings(lm_fwd), FlairEmbeddings(lm_bwd)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXcHDMK03AUo"
   },
   "source": [
    "### Sequence Tagger, Model Trainer & Evaluation\n",
    "Since we use CV to evaluate model performance, we will first define a function that takes the *result* from  *model.evaluate()* and returns three pandas dataframes\n",
    "\n",
    "\n",
    "*   ***tag_tfpn***: true-positive, false-postive, false-negative counts for each tag\n",
    "\n",
    "*   ***tag_metrics***: values for precision, recall, accuracy and f1-scores for each tag\n",
    "*   ***metrics***: values for precision, recall and f1-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MOVLPwwDKIwr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def result_summary(result):\n",
    "    scores = []\n",
    "    lines = result.detailed_results.split('\\n')\n",
    "    for line in lines[3:]:\n",
    "        split_line = re.split('\\ -\\ |\\ +|:\\ ', line)\n",
    "        scores.append(split_line)    \n",
    "    scores = np.array(scores)\n",
    "    tags = scores[:,0].tolist()\n",
    "    scores_ = scores[:, 2::2]\n",
    "    tag_tfpn = scores_[:, :3].astype(int)\n",
    "    tag_metrics = scores_[:, 4:].astype(float)\n",
    "    metrics = np.array(result.log_line.split('\\t')).astype(float).reshape(1,3)\n",
    "  \n",
    "    df_tag_tfpn = pd.DataFrame(data=tag_tfpn,index=tags,columns=['true-positive','false-positive', 'false-negative'])\n",
    "    df_tag_metrics = pd.DataFrame(data=tag_metrics,index=tags,columns=['precision','recall', 'accuracy','f1-score'])\n",
    "    df_metrics = pd.DataFrame(data=metrics, index=None,columns=['precision','recall','f1-score'])\n",
    "  \n",
    "    return df_tag_tfpn, df_tag_metrics, df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hny07e5Fktu8"
   },
   "source": [
    "Next, we will perform the actual Cross-Validation: For every Fold, we will set *corpus.test* and *corpus.train* to the respective subset of *TRAIN*. \n",
    "\n",
    "**WARNING:** This operation changes the type for *corpus.test* and *corpus.train* from *ColumnDataset* to *List of Sentences*. Training the model will not be affedted by this, but other corpus specifiv functions might not work.\n",
    "\n",
    "We initialize *SequenceTagger* and *ModelTrainer* like before, and set train_with_dev=True. After each fold, we evaluate the model on the current test set. In the end, we get three dataframes:\n",
    "\n",
    "*   ***tag_tfpn_sum***: the sum of the true-false-positive-negative values for each tag\n",
    "*   ***tag_metrics_avg***: the average precision-recall-accuracy-f1score values for each tag\n",
    "*   ***metrics_avg***: the average precision-recall-f1score\n",
    "\n",
    "These dataframes are stored as pickle files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MuNooAy-Icgq"
   },
   "outputs": [],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "from flair.datasets import DataLoader\n",
    "from flair.models import SequenceTagger\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Set number of splits\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "# Cross-Validation\n",
    "i=1\n",
    "for train_index, test_index in kf.split(TRAIN):\n",
    "    \n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"Fold:\", i)\n",
    "    corpus._test = (TRAIN[test_index]).tolist()\n",
    "    corpus._train = (TRAIN[train_index]).tolist()\n",
    "    print(corpus)\n",
    "  \n",
    "    # Initialize Sequence Tagger\n",
    "    tagger = SequenceTagger(hidden_size=512,\n",
    "                            embeddings=embeddings,\n",
    "                            tag_dictionary=tag_dictionary,\n",
    "                            tag_type=tag_type)\n",
    "  \n",
    "    # Initialize ModelTrainer\n",
    "    trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "    # Define output path\n",
    "    model_folder = \"ner-model-CV-test\"\n",
    "    model_path = f\"{ner_path}models/{model_folder}/\"\n",
    "  \n",
    "    # Training\n",
    "    trainer.train(model_path,\n",
    "                  learning_rate=0.5,\n",
    "                  anneal_factor=0.5,\n",
    "                  mini_batch_size=8,\n",
    "                  patience=50,\n",
    "                  max_epochs=1,\n",
    "                  train_with_dev=True,\n",
    "                  monitor_test=True,\n",
    "                  shuffle=False,\n",
    "                  save_final_model=False,)\n",
    "  \n",
    "    # Evaluation\n",
    "    result, eval_loss = trainer.model.evaluate(DataLoader(trainer.corpus.test,\n",
    "                                                          batch_size=8,\n",
    "                                                          num_workers=4))\n",
    "    # tag_tfpn, tag_metrics, metrics\n",
    "    if i==1:\n",
    "        tt, tm, m = result_summary(result)\n",
    "    else:\n",
    "        tt_, tm_, m_ = result_summary(result)\n",
    "        tt = tt.append(tt_)\n",
    "        tm = tm.append(tm_)\n",
    "        m = m.append(m_)\n",
    "    i+=1  \n",
    "\n",
    "tag_tfpn_sum = tt.groupby(tt.index).sum()\n",
    "tag_metrics_avg = tm.groupby(tm.index).mean()\n",
    "metrics_avg = m.mean()\n",
    "\n",
    "\n",
    "# pickle dump\n",
    "import pickle\n",
    "pickle.dump(tag_tfpn_sum,open(model_path+'tag_tfpn_sum.pkl', 'wb'))\n",
    "pickle.dump(tag_metrics_avg,open(model_path+'tag_metrics_avg.pkl', 'wb'))\n",
    "pickle.dump(metrics_avg,open(model_path+'metrics_avg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_HrF1UFDySV5"
   },
   "outputs": [],
   "source": [
    "print(metrics_avg)\n",
    "print('\\n-------------------------------------------------------------------\\n')\n",
    "print(tag_tfpn_sum)\n",
    "print('\\n-------------------------------------------------------------------\\n')\n",
    "print(tag_metrics_avg)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "named-entity-recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
